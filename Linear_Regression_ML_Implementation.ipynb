{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TISOlQJAQQdM"
      },
      "source": [
        "#**Loading the Dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VjoXF_cbDA-n"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSDrxrv-HtEN"
      },
      "source": [
        "***%matplotlib inline***\n",
        "\n",
        "This is a Jupyter Notebook magic command (not standard Python).\n",
        " It tells Jupyter to display any Matplotlib plots directly inside the notebook, right below the code cell.\n",
        " Without this, plots might open in a separate window (depending on your environment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EoJw1qCEKMl"
      },
      "outputs": [],
      "source": [
        "# from sklearn.datasets import load_boston\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=r\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2xP_3WKLU0k"
      },
      "source": [
        "***In scikit-learn***, datasets like Iris or Boston are often returned as a sklearn.utils.Bunch object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjPkwyvbJ9MB"
      },
      "outputs": [],
      "source": [
        "# Wrap the dataset into a sklearn.utils.Bunch object\n",
        "# so it behaves like sklearn's built-in datasets (e.g., load_iris).\n",
        "# - data: feature matrix (506 samples × 13 features)\n",
        "# - target: median house value in $1000s\n",
        "# - feature_names: names of the 13 features\n",
        "# - DESCR: short description of the dataset\n",
        "from sklearn.utils import Bunch\n",
        "boston = Bunch(\n",
        "    data = data,\n",
        "    target = target,\n",
        "    feature_names = [\n",
        "        \"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\",\n",
        "        \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"\n",
        "    ],\n",
        "    DESCR=\"Boston Housing dataset\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6icO_9MLNfW"
      },
      "outputs": [],
      "source": [
        "print(boston.data)\n",
        "pd.DataFrame(boston.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmkucy_dPv6X"
      },
      "source": [
        "#**Preparing the Dataset**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMZQENi6QN69"
      },
      "outputs": [],
      "source": [
        "dataset = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
        "dataset[\"PRICE\"] = boston.target\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_Nfnr3vSoXt"
      },
      "outputs": [],
      "source": [
        "dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eENcEXevSxBF"
      },
      "outputs": [],
      "source": [
        "## Summarizing the stats of the data\n",
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fgfpg4OYS_fi"
      },
      "outputs": [],
      "source": [
        "## Check the messing value\n",
        "dataset.isnull().sum() #  calcule the sum of each  colomns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY0CjDuVadYZ"
      },
      "source": [
        "##**Analyzing The Correlated Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69qSgZZPW-xd"
      },
      "source": [
        "**why correlation is very important in any Linear Regression Problem?**\n",
        "\n",
        "This goes to the heart of linear regression.\n",
        "\n",
        "Correlation is important in linear regression because:\n",
        "\n",
        "🔹 1. Linear regression assumes a linear relationship\n",
        "\n",
        "Linear regression tries to fit a straight line between the **independent variable(s) (X)** and **the dependent variable (y)**.\n",
        "\n",
        "If there’s no correlation (or very weak), a straight line won’t describe the data well → predictions will be poor.\n",
        "\n",
        "If there’s a strong correlation, linear regression can capture that relationship effectively.\n",
        "\n",
        "🔹 2. Correlation helps identify useful predictors\n",
        "\n",
        "Variables highly correlated with the target are usually better predictors.\n",
        "\n",
        "If correlation between X and y is close to 0, including that variable may not improve the model.\n",
        "\n",
        "🔹 3. Correlation reveals multicollinearity\n",
        "\n",
        "When two independent variables are highly correlated with each other, it causes multicollinearity.\n",
        "\n",
        "In regression, this makes coefficient estimates unstable and hard to interpret (the model can’t tell which variable really explains the change in y).\n",
        "\n",
        "Example: in the Boston dataset, TAX and RAD are highly correlated. Including both can confuse the model.\n",
        "\n",
        "🔹 4. Helps with feature selection & interpretation\n",
        "\n",
        "Correlation analysis is often the first step before regression:\n",
        "\n",
        "Which features matter most for predicting y?\n",
        "\n",
        "Are some features redundant because they are strongly correlated with others?\n",
        "\n",
        "🔹 5. Relation to R² (coefficient of determination)\n",
        "\n",
        "In simple linear regression (1 feature), the square of the Pearson correlation coefficient (r²) is exactly the R² value.\n",
        "\n",
        "This means correlation directly tells you how much of the variance in y is explained by x.\n",
        "\n",
        "**Correlation ranges from -1 to 1:**\n",
        "\n",
        "Close to 1 → strong positive correlation\n",
        "\n",
        "Close to -1 → strong negative correlation\n",
        "\n",
        "Close to 0 → weak or no correlation\n",
        "\n",
        "**In short:**\n",
        "\n",
        "Correlation tells you whether linear regression is appropriate.\n",
        "\n",
        "Strong correlation → good predictor.\n",
        "\n",
        "No correlation → regression won’t work well.\n",
        "\n",
        "High correlation between predictors → beware of multicollinearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFukvUnmTO83"
      },
      "outputs": [],
      "source": [
        "### Exploratory Data Analysis\n",
        "## Correlation\n",
        "dataset.corr() # corr_matrix\n",
        "dataset.corr()['PRICE'].sort_values(ascending=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWaQ-jwHeOjw"
      },
      "source": [
        "***seaborn (aliased as sns)***\n",
        "\n",
        "\n",
        "\n",
        ">is a Python library for statistical data visualization.\n",
        "\n",
        "It makes plots easier to create and prettier than raw Matplotlib.\n",
        "\n",
        "Common uses:\n",
        "\n",
        "Correlation heatmaps (sns.heatmap)\n",
        "\n",
        "Pairplots (sns.pairplot)\n",
        "\n",
        "Boxplots, violin plots, regression plots, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvI6X_NrdvHD"
      },
      "outputs": [],
      "source": [
        "# visualize the Correlation\n",
        "import seaborn as sns\n",
        "sns.pairplot(dataset) #automatically creates a grid of scatter plots for all pairs of variables in your DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ghi18Qs-f-YO"
      },
      "outputs": [],
      "source": [
        "sns.regplot(x='RM',y='PRICE',data = dataset) #regplot is used for visualizing the relationship between two specific variables, along with a linear regression fit line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHugHyxJb5_f"
      },
      "outputs": [],
      "source": [
        "plt.scatter(dataset['RM'],dataset['PRICE'])\n",
        "plt.xlabel('RM')\n",
        "plt.ylabel('PRICE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIl-UuFQfWpL"
      },
      "outputs": [],
      "source": [
        "## independent and dependent features\n",
        "X = dataset.iloc[:,:-1] #dataframe.iloc[rows, columns]\n",
        "y = dataset.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJzwkIx1sNU6"
      },
      "outputs": [],
      "source": [
        "## train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92FxoxRKzMKm"
      },
      "source": [
        "***StandardScaler (sklearn.preprocessing)***\n",
        "\n",
        "Purpose: Standardize features so each has mean = 0 and standard deviation = 1.\n",
        "\n",
        "Why: Helps machine learning models perform better, especially those sensitive to feature scale (linear regression with regularization, SVM, KNN, etc.).\n",
        "\n",
        "Steps:\n",
        "\n",
        "Import: from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "Create scaler object: scaler = StandardScaler()\n",
        "\n",
        "Fit & transform data: X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "fit → computes mean & std of each feature\n",
        "\n",
        "transform → standardizes the features\n",
        "**Xscaled ​= (X−mean) / std**\n",
        "\n",
        "\n",
        "Effect: Each column of X_scaled now has mean 0 and standard deviation 1.\n",
        "\n",
        "\n",
        "##StandardScaler: Detailed Explanation\n",
        "\n",
        "Purpose:\n",
        "\n",
        "Standardize features so each has mean = 0 and standard deviation = 1.\n",
        "\n",
        "Helps ML algorithms (like linear regression, SVM, KNN, regularized models) perform better and avoid bias due to feature scale differences.\n",
        "\n",
        "1️⃣ **Fit on training data**\n",
        "\n",
        "scaler.fit(X_train)\n",
        "\n",
        "fit() calculates and stores statistics from training data:\n",
        "\n",
        " → mean of feature j\n",
        " → standard deviation of feature j\n",
        "\n",
        "These are stored in scaler attributes:\n",
        "\n",
        "**scaler.mean_** → mean of each feature\n",
        "\n",
        "**scaler.scale_** → standard deviation of each feature\n",
        "\n",
        "**Key:** Fit is done only on training data to avoid leaking information from the test set.\n",
        "\n",
        "2️⃣ **Transform training and test data**\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\t​\n",
        "\n",
        "Uses the stored mean and std from training data to scale any new data (train or test).\n",
        "\n",
        "**Why not fit on test data?**\n",
        "\n",
        "Fitting on test data would compute a different mean and std, shifting the scale and introducing data leakage.\n",
        "\n",
        "Always use training statistics to ensure consistency and correct model evaluation.\n",
        "\n",
        "3️⃣ **Workflow Summary**\n",
        "\n",
        "Split data → X_train, X_test\n",
        "\n",
        "Initialize scaler → scaler = StandardScaler()\n",
        "\n",
        "Fit scaler on training data → scaler.fit(X_train)\n",
        "\n",
        "Transform training data → X_train_scaled = scaler.transform(X_train)\n",
        "\n",
        "Transform test data → X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "After scaling:\n",
        "\n",
        "Each feature has mean 0 and std 1 (training set)\n",
        "\n",
        "Test set is scaled consistently with training data\n",
        "\n",
        "4️⃣ **Benefits**\n",
        "\n",
        "Ensures features are on the same scale → faster convergence for gradient-based algorithms.\n",
        "\n",
        "Makes coefficients in linear regression more interpretable.\n",
        "\n",
        "Essential for regularized models (Ridge, Lasso)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g54Dtbcrxulv"
      },
      "outputs": [],
      "source": [
        "## Standardize the dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55ptbYOEzyrW"
      },
      "outputs": [],
      "source": [
        "X_train = scaler.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_sSe7QFz905"
      },
      "outputs": [],
      "source": [
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n893rvfHXjox"
      },
      "source": [
        "#**Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ0g898VYE2Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKyeaGo-bF6B"
      },
      "outputs": [],
      "source": [
        "regression = LinearRegression()\n",
        "regression.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYPjMo9jdZTk"
      },
      "outputs": [],
      "source": [
        "#Coefficients and the intercept\n",
        "print(regression.coef_)\n",
        "print(regression.intercept_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD99VTV7eR3m"
      },
      "outputs": [],
      "source": [
        "#on whith paramters the model has been trained\n",
        "regression.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcoruQCHefKX"
      },
      "outputs": [],
      "source": [
        "### Prediction with Test Data\n",
        "reg_pred =  regression.predict(X_test)\n",
        "reg_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOAR1KFLgvG5"
      },
      "outputs": [],
      "source": [
        "# plot a scarter plot for the prdiction\n",
        "plt.scatter(y_test,reg_pred)\n",
        "plt.xlabel('y_test')\n",
        "plt.ylabel('pred')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3Yh96XLkUrn"
      },
      "outputs": [],
      "source": [
        "#residuls\n",
        "residuls = y_test - reg_pred\n",
        "residuls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr4gtQnkmSW_"
      },
      "source": [
        "***Why plot residuals?***\n",
        "\n",
        "In linear regression, residuals should ideally follow a normal distribution centered around 0.\n",
        "\n",
        "Plotting residuals helps check assumptions of linear regression:\n",
        "\n",
        "Linearity: No systematic patterns\n",
        "\n",
        "Homoscedasticity: Equal variance across predictions\n",
        "\n",
        "Normality: Residuals approximately bell-shaped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoXi66QgkuGa"
      },
      "outputs": [],
      "source": [
        "#plot this residuals\n",
        "sns.displot(residuls,kind='kde')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB38ErVAqvTI"
      },
      "source": [
        "####***residuals VS predictions scatter plot :***\n",
        "\n",
        " which is another key diagnostic tool in regression. Let’s break it down:\n",
        "\n",
        "plt.scatter(reg_pred, residuls)\n",
        "\n",
        "\n",
        "**Why we plot this**\n",
        "\n",
        "This plot helps check assumptions of linear regression:\n",
        "\n",
        "Linearity: Residuals should be randomly scattered around 0 (no pattern).\n",
        "\n",
        "Homoscedasticity: Spread of residuals should be roughly the same across all predicted values.\n",
        "\n",
        "Outliers: Points far from 0 are potential outliers.\n",
        "\n",
        "✅ Ideal pattern: cloud of points, evenly spread, centered at 0.\n",
        "\n",
        "❌ Bad signs: clear patterns (like curves, funnel shapes), meaning model assumptions are violated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frYo3LNWmeqY"
      },
      "outputs": [],
      "source": [
        "## Scatter plot with respect to prdiction and residuals\n",
        "plt.scatter(reg_pred,residuls)\n",
        "plt.xlabel('pred')\n",
        "plt.ylabel('res')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4NZceJNv5CH"
      },
      "source": [
        "##**Error Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfMnCpv2weOo"
      },
      "source": [
        "To check if it’s good or not, we compare the predictions (reg_pred) with the real values (y_test).\n",
        "For that, we use error metrics.\n",
        "\n",
        "**Metrics we are using**\n",
        "\n",
        "1- MAE (Mean Absolute Error)\n",
        "\n",
        "\n",
        "> Formula:\n",
        "\n",
        "MAE=1/n * ∑∣yi−y^i∣\n",
        "\n",
        "> It’s the average absolute error. Easy to understand: “On average, the model is off by this much.”\n",
        "\n",
        "2- MSE (Mean Squared Error)\n",
        ">Formula:\n",
        "\n",
        "MSE= 1/n * ∑((yi​−y^​i​)^2) avec 1<=i<=n\n",
        "\n",
        "> Similar to MAE, but it squares the errors. This punishes big mistakes more.\n",
        "\n",
        "3- RMSE (Root Mean Squared Error)\n",
        ">Formula:\n",
        "\n",
        "𝑅𝑀𝑆𝐸=sqrt(𝑀𝑆𝐸)\n",
        "\n",
        "\n",
        "\n",
        ">It’s just the square root of MSE, which brings the error back to the same units as your target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShAycz3uugfp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "print(mean_absolute_error(y_test,reg_pred))\n",
        "print(mean_squared_error(y_test,reg_pred))\n",
        "print(np.sqrt(mean_squared_error(y_test,reg_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p-Ji_Xu1QiO"
      },
      "source": [
        "###**R square and adjusted R square**\n",
        "####****R square**\n",
        ">Formula : R2 = 1 - SSR / SST\n",
        "\n",
        "R^2 : coefficient of determination\n",
        "\n",
        "SSR : sum of squares of residuals\n",
        "\n",
        "SST : total sum of squares\n",
        "\n",
        "####****Adjust R2**\n",
        ">Formula : Adjust R2 = [(1-R2)*(n-1)/(n-k-1)]\n",
        "\n",
        "n : nb of observations\n",
        "\n",
        "k : the nb of predictor variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivPVHbq003JE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "score = r2_score(y_test,reg_pred)\n",
        "score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_huFdmci3DOY"
      },
      "outputs": [],
      "source": [
        "n = len(y_test)\n",
        "k = X_test.shape[1]\n",
        "[(1-score)*(n-1)/(n-k-1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR0wXDZUGAn7"
      },
      "source": [
        "#**New Data Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7GNeEImGFmI"
      },
      "outputs": [],
      "source": [
        "boston.data[0].reshape(1,-1).shape\n",
        "# reshape(rows,colomuns)\n",
        "# 1 means we want 1 row.\n",
        "# -1 tells NumPy: “Automatically calculate the number of columns based on the original size.”\n",
        "#boston.data[2].shape #(rows,columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-jxviB6G9Ag"
      },
      "outputs": [],
      "source": [
        "scaler.transform(boston.data[0].reshape(1,-1)) # transform the new data\n",
        "regression.predict(scaler.transform(boston.data[0].reshape(1,-1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6qTdjnsPfGm"
      },
      "source": [
        "#**Pickling The Model file For Deployment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjeSbr-9QXXt"
      },
      "source": [
        ">***What is Pickling?***\n",
        "\n",
        "Pickling means saving (serializing) a Python object — such as your trained ML model — to a binary file on disk so you can reuse it later without retraining.\n",
        "\n",
        "The saved file is called a pickle file and usually has the extension .pkl or .sav.\n",
        "\n",
        ">***Why is it important?***\n",
        "\n",
        "Because after you train a model:\n",
        "\n",
        "You don’t want to retrain it every time you want to use it.\n",
        "\n",
        "You might want to deploy it in a web app, desktop app, or API.\n",
        "\n",
        "Pickling allows you to save the trained model once, and later load it instantly to make predictions.\n",
        "\n",
        ">***Library used: pickle***\n",
        "\n",
        "Python’s built-in module pickle is used for this.\n",
        "\n",
        "🔹 Example\n",
        "import pickle\n",
        "\n",
        "> Save (Pickle) the model\n",
        "\n",
        "pickle.dump(regressor, open('reg_model.pkl', 'wb'))\n",
        "\n",
        "Explanation:\n",
        "\n",
        "pickle.dump(obj, file) saves an object to a file.\n",
        "\n",
        "- 'wb' means write binary mode.\n",
        "- 'reg_model.pkl' is the filename.\n",
        "\n",
        ">Load (Unpickle) the model later\n",
        "- Load the model from disk\n",
        "loaded_model = pickle.load(open('reg_model.pkl', 'rb'))\n",
        "\n",
        "- Use it for prediction\n",
        "\n",
        "predictions = loaded_model.predict(X_test)\n",
        "Explanation:\n",
        "- 'rb' = read binary mode.\n",
        "\n",
        "\n",
        "loaded_model is now your trained model, ready to use.\n",
        "\n",
        "\n",
        "pickle is the Python library that performs these two actions\n",
        "\n",
        ">***Serialization*** = converting a Python object into a binary format (bytes)\n",
        "so it can be saved or transmitted.\n",
        "\n",
        ">***Deserialization*** = converting that binary data back into the original object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTNbVBUFMeu5"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIN8ROuLPvHp"
      },
      "outputs": [],
      "source": [
        "pickle.dump(regression,open('regmodel.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJRMqZPfRnst"
      },
      "outputs": [],
      "source": [
        "pickle_model = pickle.load(open('regmodel.pkl','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lllFR9q_UBNk"
      },
      "outputs": [],
      "source": [
        "## prediction\n",
        "pickle_model.predict(scaler.transform(boston.data[0].reshape(1,-1)))\n",
        "#pickle_model.predict(scaler.transform(boston.data[0].reshape(1,-1))) - boston.target[0]\n",
        "# When it’s “not ok”\n",
        "# If you see:\n",
        "# Residuals much larger than your RMSE (e.g., 20 or 30)\n",
        "# A consistent positive/negative pattern (model always too high or too low)\n",
        "# then your model might not be well-fitted (maybe missing a key feature, or needs nonlinear terms)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TISOlQJAQQdM",
        "Fmkucy_dPv6X"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.13.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
